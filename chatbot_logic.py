import google.generativeai as genai
import PyPDF2
import os
from sentence_transformers import SentenceTransformer
import numpy as np

# ================== Embedding Model ==================
embed_model = SentenceTransformer(
    "paraphrase-multilingual-MiniLM-L12-v2"
)

# ================== Gemini API ==================
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel("gemini-3-flash-preview")
chat = model.start_chat(history=[])

# ================== PDF PATH ==================
PDF_PATH = os.path.join(os.path.dirname(__file__), "scout.pdf")

# ================== Load PDF ==================
def load_pdf_chunks(path, chunk_size=800):
    text = ""
    with open(path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            if page.extract_text():
                text += page.extract_text() + "\n"

    chunks = []
    current = ""
    for line in text.split("\n"):
        current += line + " "
        if len(current) >= chunk_size:
            chunks.append(current.strip())
            current = ""
    if current.strip():
        chunks.append(current.strip())

    return chunks

try:
    pdf_chunks = load_pdf_chunks(PDF_PATH)
    print("PDF Loaded:", len(pdf_chunks))
except Exception as e:
    print("PDF ERROR:", e)
    pdf_chunks = []

# ================== Embeddings ==================
if pdf_chunks:
    chunk_embeddings = embed_model.encode(
        pdf_chunks, normalize_embeddings=True
    )
else:
    chunk_embeddings = np.array([])

def semantic_search(question, chunks, embeddings, top_k=10):
    if embeddings.size == 0:
        return []

    q_vec = embed_model.encode(
        [question], normalize_embeddings=True
    )[0]
    scores = np.dot(embeddings, q_vec)
    top_indices = np.argsort(scores)[-top_k:][::-1]
    return [chunks[i] for i in top_indices if scores[i] > 0.12]

# ================== Keyword Search ==================
def normalize_text(text):
    stop_words = [
        "ูุง", "ูู", "ูู", "ูู", "ุนู", "ูู",
        "ุงุฐูุฑ", "ุนุฑู", "ุชุนุฑูู", "ุนุฏุฏ", "ูู",
        "ูุชู", "ุฃูู", "ููุงุฐุง", "ููู",
        "ูุงูู", "ูุงูู", "ูู", "ุจู", "ุจูุง",
        "ูุถุญ", "ุงุดุฑุญ", "ูุงุฑู", "ุฑุชุจ"
    ]

    words = text.replace("ุ", "").split()
    return [w for w in words if w not in stop_words]

synonyms = {
    "ุฃููุงุน": ["ุชููุณู", "ุชุตูู", "ุฃูุณุงู"],
    "ุชุนุฑูู": ["ูุง ูู", "ูุงูู", "ุนุฑูู"],
    "ุฃูุฏุงู": ["ุงููุฏู", "ุบุงูุงุช", "ุฃูููุฉ"],
    "ูุจุงุฏุฆ": ["ุฃุณุณ", "ููู"],
    "ุทุฑููุฉ": ["ุงูุทุฑููุฉ ุงููุดููุฉ", "ูุธุงู"],

    "ูุคุณุณ": ["ุฃูุดุฃ", "ุชุฃุณุณุช", "ูุคุณุณ ุงูุญุฑูุฉ"],
    "ุญุฑูุฉ": ["ุงูุญุฑูุฉ ุงููุดููุฉ", "ุงููุดุงูุฉ"],
    "ุบูุฑ ุณูุงุณูุฉ": ["ูุง ุชูุญุงุฒ", "ุบูุฑ ุญุฒุจูุฉ"],

    "ูุนุฏ": ["ุงููุนุฏ ุงููุดูู", "ูุชุนูุฏ", "ุงูุชุนูุฏ"],
    "ูุงููู": ["ูุงููู ุงููุดุงูุฉ", "ููุงููู"],

    "ูุฑุงุญู": ["ุงููุฑุงุญู ุงููุดููุฉ", "ูุณุชููุงุช"],
    "ุจุฑุงุนู": ["ุงูุจุฑุงุนู"],
    "ุฃุดุจุงู": ["ุงูุฃุดุจุงู", "ุดุจุงู"],
    "ูุดุงูุฉ": ["ุงููุดุงู", "ุงููุดุงูุฉ"],
    "ุฌูุงูุฉ": ["ุงูุฌูุงูุฉ"],

    "ุทููุนุฉ": ["ุงูุทููุนุฉ", "ุฑูุท", "ุณุฏุงุณู"],
    "ุนุฑูู": ["ุนุฑูู ุงูุทููุนุฉ"],
    "ูุณุฆูููุงุช": ["ููุงู", "ูุงุฌุจุงุช"],

    "ุชุญูุฉ": ["ุงูุชุญูุฉ ุงููุดููุฉ"],
    "ุนูุงูุฉ": ["ุงูุนูุงูุฉ ุงููุดููุฉ"],

    "ุดุนุงุฑ": ["ุดุนุงุฑ ุงููุดุงูุฉ", "ุฑูุฒ"],
    "ุฌูุน": ["ุชูุงููุฏ ุงูุฌูุน", "ุงูุชุฌูุน"],
    "ูุฏุงุก": ["ุงููุฏุงุก", "ุฃุณุงููุจ ุงููุฏุงุก"],

    "ุฅุณุนุงูุงุช": ["ุงูุฅุณุนุงูุงุช ุงูุฃูููุฉ"],
    "ูุฎููุงุช": ["ุงูุชุฎููู", "ุงููุฎููุงุช"],
    "ุงุชุฌุงูุงุช": ["ุงูุฌูุงุช", "ุชุญุฏูุฏ ุงูุงุชุฌุงู"],
    "ุดูุฑุงุช": ["ุงูุดูุฑุฉ", "ุงูุดูุฑุงุช"]
}

def expand_keywords(words):
    expanded = set(words)
    for w in words:
        if w in synonyms:
            expanded.update(synonyms[w])
    return expanded

def find_relevant_chunks(question, chunks, max_chunks=5):
    keywords = expand_keywords(normalize_text(question))
    scored_chunks = []

    for chunk in chunks:
        score = 0
        for word in keywords:
            if word in chunk:
                score += chunk.count(word) * 2
        if score > 0:
            scored_chunks.append((score, chunk))

    scored_chunks.sort(key=lambda x: x[0], reverse=True)
    return [c[1] for c in scored_chunks[:max_chunks]]

def is_list_question(question):
    keywords = [
        "ุงุฐูุฑ", "ุนุฏุฏ", "ูุง ูู", "ูุง ูู",
        "ุฃููุงุน", "ุฃูุณุงู", "ูุงููู", "ูุฑุงุญู"
    ]
    return any(k in question for k in keywords)

def extract_list_items(chunks):
    items = []
    for chunk in chunks:
        for line in chunk.split("\n"):
            line = line.strip()
            if (
                line.startswith("โข")
                or line.startswith("-")
                or line.startswith("โ")
                or line[:2].isdigit()
            ):
                items.append(line)

    seen = set()
    final_items = []
    for item in items:
        if item not in seen:
            seen.add(item)
            final_items.append(item)

    return final_items

# ================== MAIN ANSWER FUNCTION ==================
def answer_question(user_input: str) -> str:
    if not user_input.strip():
        return "โ ุงูุณุคุงู ูุงุฑุบ"

    if not pdf_chunks:
        return "โ ูู ูุชู ุชุญููู ุงููููุฌ"

    relevant_text = semantic_search(
        user_input, pdf_chunks, chunk_embeddings, top_k=12
    )

    if not relevant_text:
        relevant_text = find_relevant_chunks(
            user_input, pdf_chunks
        )

    if not relevant_text:
        return "โ๏ธ ูุฐู ุงููุนูููุฉ ุบูุฑ ููุฌูุฏุฉ ูู ุงููููุฌ"

    prompt = f"""
ุฃูุช ูุงุฆุฏ ุนุงู ุฎุจูุฑ ูู ุงููุดุงูุฉ.
ูููุชู ุดุฑุญ ุงููููุฌ ูููุฎุฏูู.

๐ ูุต ูู ุงููููุฌ:
{' '.join(relevant_text)}

โ ุณุคุงู ุงูุทุงูุจ:
{user_input}

๐ ููุงุนุฏ ุตุงุฑูุฉ:
- ุฃุฌุจ ูู ุงููุต ููุท
- ูุง ุชุถู ุฃู ูุนูููุฉ ุฎุงุฑุฌ ุงููููุฌ
- ุงูุชุจ ุงูุฅุฌุงุจุฉ ุฃูููุง ููุง ูุฑุฏุช ูู ุงููููุฌ
- ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ุชุนุฏุงุฏูุง ูุงุฐูุฑ ุฌููุน ุงูุจููุฏ ูุงููุฉ ุฏูู ุงุฎุชุตุงุฑ
- ุจุนุฏูุง ูุฏูู ุดุฑุญูุง ูุจุณุทูุง ูููุฎุฏูู
- ูู ูู ุชุฌุฏ ุฅุฌุงุจุฉ ูุงุถุญุฉ ูู:
โ๏ธ ูุฐู ุงููุนูููุฉ ุบูุฑ ููุฌูุฏุฉ ูู ุงููููุฌ
- ุงูุนุฑุจูุฉ ููุท
"""

    try:
        response = chat.send_message(prompt)
        return response.text
    except Exception as e:
        return f"โ ุฎุทุฃ ูู ุงูุงุชุตุงู ุจุงููููุฐุฌ: {e}"
